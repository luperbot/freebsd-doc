<?xml version="1.0" encoding="iso-8859-1" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /><title>21.8. ZFS-Eigenschaften und Terminologie</title><link rel="stylesheet" type="text/css" href="docbook.css" /><link rev="made" href="mailto:doc@FreeBSD.org" /><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><link rel="home" href="index.html" title="Das FreeBSD-Handbuch" /><link rel="up" href="zfs.html" title="Kapitel 21. Das Z-Dateisystem (ZFS)" /><link rel="prev" href="zfs-links.html" title="21.7. Zusätzliche Informationen" /><link rel="next" href="filesystems.html" title="Kapitel 22. Dateisystemunterstützung" /><link rel="copyright" href="legalnotice.html" title="Rechtlicher Hinweis" /><script xmlns="" type="text/javascript" src="/layout/js/google.js"></script></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">21.8. <acronym class="acronym">ZFS</acronym>-Eigenschaften und
      Terminologie</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="zfs-links.html">Zurück</a> </td><th width="60%" align="center">Kapitel 21. Das Z-Dateisystem (<acronym class="acronym">ZFS</acronym>)</th><td width="20%" align="right"> <a accesskey="n" href="filesystems.html">Weiter</a></td></tr></table><hr /></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="zfs-term"></a>21.8. <acronym class="acronym">ZFS</acronym>-Eigenschaften und
      Terminologie</h2></div></div></div><p><acronym class="acronym">ZFS</acronym> ist ein fundamental anderes
      Dateisystem aufgrund der Tatsache, dass es mehr als ein
      Dateisystem ist.  <acronym class="acronym">ZFS</acronym> kombiniert die Rolle
      eines Dateisystems mit dem Volumemanager, was es ermöglicht,
      zusätzliche Speichermedien zu einem laufenden System
      hinzuzufügen und diesen neuen Speicher sofort auf allen auf dem
      Pool existierenden Dateisystemen zur Verfügung zu haben.  Durch
      die Kombination von traditionell getrennten Rollen ist
      <acronym class="acronym">ZFS</acronym> in der Lage, Einschränkungen, die zuvor
      <acronym class="acronym">RAID</acronym>-Gruppen daran gehindert hatten, zu
      wachsen.  Jedes Gerät auf höchster Ebene in einem Pool wird ein
      <span class="emphasis"><em>vdev</em></span> genannt, was eine einfache Platte oder
      eine <acronym class="acronym">RAID</acronym>-Transformation wie ein Spiegel oder
      <acronym class="acronym">RAID-Z</acronym>-Verbund sein kann.
      <acronym class="acronym">ZFS</acronym>-Dateisysteme
      (<span class="emphasis"><em>datasets</em></span> genannt), haben jeweils Zugriff
      auf den gesamten freien Speicherplatz des gesamten Pools.  Wenn
      Blöcke aus diesem Pool allokiert werden, verringert sich auch
      der freie Speicherplatz für jedes Dateisystem.  Dieser Ansatz
      verhindert die allgegenwärtige Falle von umfangreichen
      Partitionen, bei denen freier Speicherplatz über alle
      Partitionen hinweg fragmentiert wird.</p><div class="informaltable"><table width="100%" border="1"><colgroup><col /><col /></colgroup><tbody valign="top"><tr><td valign="top"><a id="zfs-term-zpool"></a>zpool</td><td valign="top">Ein Speicher-<span class="emphasis"><em>Pool</em></span> ist der
	      grundlegendste Baustein von <acronym class="acronym">ZFS</acronym>.  Ein
	      Pool besteht aus einem oder mehreren vdevs, was die
	      zugrundeliegenden Geräte repräsentiert, welche die Daten
	      speichern.  Ein Pool wird dann verwendet, um ein oder
	      mehrere Dateisysteme (Datasets) oder Blockgeräte
	      (Volumes) zu erstellen.  Diese Datasets und Volumes
	      teilen sich den im Pool verfügbaren freien
	      Speicherplatz.  Jeder Pool wird eindeutig identifiziert
	      durch einen Namen und eine <acronym class="acronym">GUID</acronym>.  Die
	      verfügbaren Eigenschaften werden durch die
	      <acronym class="acronym">ZFS</acronym>-Versionsnummer des Pool
	      bestimmt.

	      <div xmlns="" class="note"><h3 class="admontitle">Anmerkung: </h3><p xmlns="http://www.w3.org/1999/xhtml">FreeBSD 9.0 und 9.1 enthalten Unterstützung
		  für <acronym class="acronym">ZFS</acronym> Version 28.  Spätere
		  Versionen setzen <acronym class="acronym">ZFS</acronym> Version 5000
		  mit Feature Flags ein.  Das neue Feature Flag System
		  erlaubt eine größere Kompatibilität mit anderen
		  Implementierungen von <acronym class="acronym">ZFS</acronym>.</p></div>
	    </td></tr><tr><td valign="top"><a id="zfs-term-vdev"></a>vdev Arten</td><td valign="top">Ein Pool besteht aus einem oder mehreren vdevs, die
	      selbst eine einfache Platte oder im Fall von
	      <acronym class="acronym">RAID</acronym> eine Gruppe von Platten
	      darstellt.  Wenn mehrere vdevs eingesetzt werden,
	      verteilt <acronym class="acronym">ZFS</acronym> die Daten über die
	      vdevs, um die Geschwindigkeit zu steigern und den
	      verfügbaren Platz zu maximieren.

	      <div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><a id="zfs-term-vdev-disk"></a><span class="emphasis"><em>Festplatte</em></span>
		    - Der einfachste Typ von vdev ist ein
		    Standard-Blockgerät.  Dies kann die komplette
		    Platte (wie
		    <code class="filename"><em class="replaceable"><code>/dev/ada0</code></em></code>
		    oder
		    <code class="filename"><em class="replaceable"><code>/dev/da0</code></em></code>)
		    oder auch eine Partition
		    (<code class="filename"><em class="replaceable"><code>/dev/ada0p3</code></em></code>)
		    sein.
		    Auf FreeBSD gibt es keine Geschwindigkeitseinbußen
		    bei der Verwendung einer Partition anstatt einer
		    kompletten Platte.  Dies unterscheidet sich von
		    den Empfehlungen, welche in der Solaris
		    Dokumentation gegeben werden.</p></li><li class="listitem"><p><a id="zfs-term-vdev-file"></a><span class="emphasis"><em>File</em></span>
		    - Zusätzlich zu Festplatten können
		    <acronym class="acronym">ZFS</acronym>-Pools aus regulären Dateien
		    aufgebaut sein, was besonders hilfreich ist, um zu
		    testen und zu experimentieren.  Verwenden Sie den
		    kompletten Pfad zu der Datei als Gerätepfad im
		    Befehl zpool create.  Alle vdevs müssen mindestens
		    128 MB groß sein.</p></li><li class="listitem"><p><a id="zfs-term-vdev-mirror"></a><span class="emphasis"><em>Mirror</em></span>
		    - Wenn ein Spiegel erstellt wird, verwenden Sie
		    das Schlüsselwort <code class="literal">mirror</code>,
		    gefolgt von der Liste an Mitgliedsgeräten für den
		    Spiegel.  Ein Spiegel besteht aus zwei oder mehr
		    Geräten und sämtliche Daten werden auf alle
		    Geräte, die Mitglied des Spiegels sind,
		    geschrieben.  Ein Spiegel-vdev wird nur soviele
		    Daten speichern, wie das kleinste Gerät im Verbund aufnehmen
		    kann.  Ein Spiegel-vdev kann den Verlust von allen
		    Mitgliedsgeräten bis auf eines verkraften, ohne
		    irgendwelche Daten zu verlieren.</p><div xmlns="" class="note"><h3 class="admontitle">Anmerkung: </h3><p xmlns="http://www.w3.org/1999/xhtml">Ein reguläre einzelne vdev-Platte kann
		      jederzeit zu einem Spiegel-vdev über das
		      Kommando <code class="command">zpool <a class="link" href="zfs-zpool.html#zfs-zpool-attach" title="21.3.2. Hinzufügen und Löschen von Geräten">attach</a></code>
		      aktualisiert werden.</p></div></li><li class="listitem"><p><a id="zfs-term-vdev-raidz"></a><span class="emphasis"><em><acronym class="acronym">RAID-Z</acronym></em></span>
		    - <acronym class="acronym">ZFS</acronym> implementiert
		    <acronym class="acronym">RAID-Z</acronym>, eine Varianten des
		    <acronym class="acronym">RAID-5</acronym>-Standards, der bessere
		    Verteilung der Parität bietet und das
		    <span class="quote">&#8222;<span class="quote"><acronym class="acronym">RAID-5</acronym> write
		    hole</span>&#8220;</span> eliminiert, bei dem die Daten und
		    Parität nach einem unerwarteten Neustart
		    inkonsistent werden können.
		    <acronym class="acronym">ZFS</acronym> unterstützt drei Stufen von
		    <acronym class="acronym">RAID-Z</acronym>, die unterschiedliche
		    Arten von Redundanz im Austausch gegen niedrigere
		    Stufen von verwendbarem Speicher.
		    Diese Typen werden <acronym class="acronym">RAID-Z1</acronym>
		    bis <acronym class="acronym">RAID-Z3</acronym> genannt, basierend
		    auf der Anzahl der Paritätsgeräte im Verbund und
		    der Anzahl an Platten, die ausfallen können,
		    während der Pool immer noch normal
		    funktioniert.</p><p>In einer
		    <acronym class="acronym">RAID-Z1</acronym>-Konfiguration mit
		    vier Platten, bei der jede 1 TB besitzt,
		    beträgt der verwendbare Plattenplatz 3 TB und
		    der Pool wird immer noch im Modus degraded
		    weiterlaufen, wenn eine Platte davon ausfällt.
		    Wenn eine zusätzliche Platte ausfällt, bevor die
		    defekte Platte ersetzt wird, können alle Daten im
		    Pool verloren gehen.</p><p>Eine Konfiguration von acht Platten zu je
		    1 TB als <acronym class="acronym">RAID-Z3</acronym> wird
		    5 TB verwendbaren Speicher bieten und in der
		    Lage sein, weiterhin zu funktionieren, wenn drei
		    Platten ausgefallen sind.  <span class="trademark">Sun</span>&#8482; empfiehlt nicht
		    mehr als neun Platten in einem einzelnen vdev.
		    Wenn die Konfiguration mehr Platten aufweist, wird
		    empfohlen, diese in getrennten vdevs aufzuteilen,
		    so dass die Daten des Pools zwischen diesen
		    aufgeteilt werden.</p><p>Eine Konfiguration von zwei
		    <acronym class="acronym">RAID-Z2</acronym>-vdevs, bestehend aus
		    jeweils 8 Platten würde etwa einem
		    <acronym class="acronym">RAID-60</acronym>-Verbund entsprechen.
		    Der Speicherplatz einer
		    <acronym class="acronym">RAID-Z</acronym>-Gruppe ist ungefähr die
		    Größe der kleinsten Platte multipliziert mit der
		    Anzahl von nicht-Paritätsplatten.  Vier 1 TB
		    Platten in einem <acronym class="acronym">RAID-Z1</acronym>
		    besitzt eine effektive Größe von ungefähr
		    3 TB und ein Verbund von acht
		    1 TB-Platten als <acronym class="acronym">RAID-Z3</acronym>
		    enthält 5 TB verfügbarer Plattenplatz.</p></li><li class="listitem"><p><a id="zfs-term-vdev-spare"></a><span class="emphasis"><em>Spare</em></span>
		    - <acronym class="acronym">ZFS</acronym> besitzt einen speziellen
		    Pseudo-vdev Typ, um einen Überblick über die
		    verfügbaren hot spares zu behalten.  Beachten Sie,
		    dass hot spares nicht automatisch eingesetzt
		    werden.  Diese müssen manuell konfiguriert werden,
		    um ein ausgefallenes Gerät über <code class="command">zfs
		      replace</code> zu ersetzen.</p></li><li class="listitem"><p><a id="zfs-term-vdev-log"></a><span class="emphasis"><em>Log</em></span>
		    - <acronym class="acronym">ZFS</acronym> Log-Geräte, auch
		    bezeichnet als ein <acronym class="acronym">ZFS</acronym> Intent
		    Log (<a class="link" href="zfs-term.html#zfs-term-zil"><acronym class="acronym">ZIL</acronym></a>)
		    verschieben das Intent Log von den regulären
		    Geräten im Pool auf ein dediziertes Gerät,
		    typischerweise eine <acronym class="acronym">SSD</acronym>.  Ein
		    dediziertes Log-Gerät zu besitzen kann die
		    Geschwindigkeit von Anwendungen mit einer großen
		    Anzahl von synchronen Schreibvorgängen, besonders
		    Datenbanken, signifikant steigern.  Log-Geräte
		    können gespiegelt werden, jedoch wird
		    <acronym class="acronym">RAID-Z</acronym> nicht unterstützt.
		    Werden mehrere Log-Geräte verwendet, so werden
		    Schreibvorgänge gleichmäßig unter diesen
		    aufgeteilt.</p></li><li class="listitem"><p><a id="zfs-term-vdev-cache"></a><span class="emphasis"><em>Cache</em></span>
		    - Ein Cache-vdev einem Pool hinzuzufügen, erhöht
		    den Speicher des <a class="link" href="zfs-term.html#zfs-term-l2arc"><acronym class="acronym">L2ARC</acronym></a>
		    Caches.  Cache-Geräte lassen sich nicht spiegeln.
		    Da ein Cache-Gerät nur zusätzliche Kopien von
		    existierenden Daten speichert, gibt es kein
		    Risiko, Daten zu verlieren.</p></li></ul></div></td></tr><tr><td valign="top"><a id="zfs-term-txg"></a>Transaktionsgruppe
	      (Transaction Group, <acronym class="acronym">TXG</acronym>)</td><td valign="top">Transaktionsgruppen sind die Art und Weise, wie
	      geänderte Blöcke zusammen gruppiert und letztendlich auf
	      den Pool geschrieben werden.  Transaktionsgruppen sind
	      die atomare Einheit, welche <acronym class="acronym">ZFS</acronym>
	      verwendet, um Konsistenz zu gewährleisten.  Jeder
	      Transaktionsgruppe wird eine einzigartige, fortlaufende
	      64-Bit Identifikationsnummer zugewiesen.  Es kann bis zu
	      drei aktive Transaktionsgruppen gleichzeitig geben,
	      wobei sich jede davon in einem der folgenden drei
	      Zustände befinden kann:

	      <div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><span class="emphasis"><em>Open (Offen)</em></span> - Wenn eine neue
		    Transaktionsgruppe erstellt wird, befindet diese
		    sich im Zustand offen und akzeptiert neue
		    Schreibvorgänge.  Es ist immer eine
		    Transaktionsgruppe in diesem Zustand, jedoch kann
		    die Transaktionsgruppe neue Schreibvorgänge
		    ablehnen, wenn diese ein Limit erreicht hat.
		    Sobald eine offene Transaktionsgruppe an das Limit
		    stößt oder das <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-txg-timeout"><code class="varname">vfs.zfs.txg.timeout</code></a>
		    wurde erreicht, geht die Transaktionsgruppe in den
		    nächsten Zustand über.</p></li><li class="listitem"><p><span class="emphasis"><em>Quiescing (Stilllegen)</em></span> -
		    Ein kurzer Zustand, der es noch ausstehenden
		    Operationen erlaubt, zum Abschluss zu kommen,
		    währenddessen das Erstellen einer neuen
		    Transaktionsgruppe jedoch nicht blockiert wird.
		    Sobald alle Transaktionen in der Gruppe
		    abgeschlossen sind, geht die Transaktionsgruppen
		    in den letzten Zustand über.</p></li><li class="listitem"><p><span class="emphasis"><em>Syncing (Sychronisieren)</em></span>
		    - Alle Daten in der Transaktionsgruppe werden auf
		    das Speichermedium geschrieben.  Dieser Prozess
		    wird wiederum andere Daten wie Metadaten und space
		    maps verändern, die ebenfalls auf das
		    Speichermedium geschrieben werden müssen.  Der
		    Prozess des Synchronisierens beinhaltet mehrere
		    Durchläufe.  Der erste Prozess, welches der
		    größte, gefolgt von den Metadaten, ist,
		    beinhaltet alle geänderten Datenblöcke und kann
		    mehrere Durchläufe benötigen, um zum Ende zu
		    gelangen.  Da das Allokieren von Speicher für die
		    Datenblöcke neue Metadaten generiert, kann der
		    Synchronisationsprozess nicht beendet werden, bis
		    ein Durchlauf fertig ist, der keinen zusätzlichen
		    Speicher allokiert.  Der Synchronisierungszustand
		    ist der Zustand, in dem auch
		    <span class="emphasis"><em>synctasks</em></span> abgeschlossen
		    werden.  Synctasks sind administrative
		    Operationen, wie das Erstellen oder zerstören von
		    Schnappschüssen und Datasets, welche den Überblock
		    verändern, wenn sie abgeschlossen sind.  Sobald
		    der Synchronisationszustand abgeschlossen ist,
		    geht die Transaktionsgruppe aus dem
		    Stilllegungszustand über in den
		    Synchronisationszustand.</p></li></ul></div>

	      Alle administrativen Funktionen, wie <a class="link" href="zfs-term.html#zfs-term-snapshot"><code class="command">Schnappschüsse</code></a>
	      werden als Teil einer Transaktionsgruppe geschrieben.
	      Wenn ein synctask erstellt ist, wird dieser der momentan
	      geöffneten Transaktionsgruppe hinzugefügt und diese
	      Gruppe wird so schnell wie möglich in den
	      Synchronisationszustand versetzt, um die Latenz von
	      administrativen Befehlen zu reduzieren.</td></tr><tr><td valign="top"><a id="zfs-term-arc"></a>Adaptive Replacement
	      Cache (<acronym class="acronym">ARC</acronym>)</td><td valign="top"><acronym class="acronym">ZFS</acronym> verwendet einen Adaptive
	      Replacement Cache (<acronym class="acronym">ARC</acronym>), anstatt
	      eines traditionellen Least Recently Used
	      (<acronym class="acronym">LRU</acronym>) Caches.  Ein
	      <acronym class="acronym">LRU</acronym>-Cache ist eine einfache Liste von
	      Elementen im Cache, sortiert nach der letzten Verwendung
	      jedes Elements in der Liste.  Neue Elemente werden an
	      den Anfang der Liste eingefügt.  Wenn der Cache voll
	      ist, werden Elemente vom Ende der Liste verdrängt, um
	      Platz für aktivere Objekte zu schaffen.  Ein
	      <acronym class="acronym">ARC</acronym> besteht aus vier Listen:
	      derjenigen der Most Recently Used
	      (<acronym class="acronym">MRU</acronym>) und Most Frequently Used
	      (<acronym class="acronym">MFU</acronym>) Objekte, plus einer sogenannten
	      ghost list für jede von beiden.  Diese Ghost Lists
	      verfolgen die kürzlich verdrängten Objekte, um zu
	      verhindern, dass diese erneut in den Cache aufgenommen
	      werden.  Dies erhöht die Trefferrate (hit ratio) des
	      Caches, indem verhindert wird, dass Elemente, die in der
	      Vergangenheit nur ab und zu benutzt wurden, wieder im
	      Cache landen.  Ein weiterer Vorteil der Verwendung
	      sowohl einer <acronym class="acronym">MRU</acronym> und einer
	      <acronym class="acronym">MFU</acronym> ist, dass das Scannen eines
	      gesamten Dateisystems normalerweise alle Daten aus einem
	      <acronym class="acronym">MRU</acronym>- oder
	      <acronym class="acronym">LRU</acronym>-Cache verdrängt, um dem gerade
	      frisch zugegriffenem Inhalt den Vorzug zu geben.  Mit
	      <acronym class="acronym">ZFS</acronym> gibt es also eine
	      <acronym class="acronym">MFU</acronym>, die nur die am häufigsten
	      verwendeten Elemente beinhaltet und der Cache von am
	      meisten zugegriffenen Blöcken bleibt erhalten.</td></tr><tr><td valign="top"><a id="zfs-term-l2arc"></a><acronym class="acronym">L2ARC</acronym></td><td valign="top"><acronym class="acronym">L2ARC</acronym> ist die zweite Stufe des
	      Caching-Systems von <acronym class="acronym">ZFS</acronym>.  Der
	      Haupt-<acronym class="acronym">ARC</acronym> wird im
	      <acronym class="acronym">RAM</acronym> abgelegt.  Da die Menge an
	      verfügbarem <acronym class="acronym">RAM</acronym> meist begrenzt ist,
	      kann <acronym class="acronym">ZFS</acronym> auch <a class="link" href="zfs-term.html#zfs-term-vdev-cache">cache vdevs</a>
	      verwenden.  Solid State Disks (<acronym class="acronym">SSD</acronym>s)
	      werden oft als diese Cache-Geräte eingesetzt, aufgrund
	      ihrer höheren Geschwindigkeit und niedrigeren Latenz im
	      Vergleich zu traditionellen drehenden Speichermedien wie
	      Festplatten.  Der Einsatz des <acronym class="acronym">L2ARC</acronym>
	      ist optional, jedoch wird durch die Verwendung eine
	      signifikante Geschwindigkeitssteigerung bei
	      Lesevorgängen bei Dateien erzielt, welche auf der
	      <acronym class="acronym">SSD</acronym> zwischengespeichert sind, anstatt
	      von der regulären Platte gelesen werden zu müssen.
	      <acronym class="acronym">L2ARC</acronym> kann ebenfalls die <a class="link" href="zfs-term.html#zfs-term-deduplication">Deduplizierung</a>
	      beschleunigen, da eine <acronym class="acronym">DDT</acronym>, welche
	      nicht in den <acronym class="acronym">RAM</acronym> passt, jedoch in den
	      <acronym class="acronym">L2ARC</acronym> wesentlich schneller sein wird
	      als eine <acronym class="acronym">DDT</acronym>, die von der Platte
	      gelesen werden muss.  Die Häufigkeit, in der Daten zum
	      Cache-Gerät hinzugefügt werden, ist begrenzt, um  zu
	      verhindern, dass eine <acronym class="acronym">SSD</acronym> frühzeitig
	      durch zu viele Schreibvorgänge aufgebraucht ist.  Bis
	      der Cache voll ist (also der erste Block verdrängt
	      wurde, um Platz zu schaffen), wird das Schreiben auf den
	      <acronym class="acronym">L2ARC</acronym> begrenzt auf die Summe der
	      Schreibbegrenzung und das Bootlimit, sowie hinterher auf
	      das Schreiblimit.  Ein paar <a class="citerefentry" href="http://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>-Werte steuert
	      diese Limits.  <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-l2arc_write_max"><code class="varname">vfs.zfs.l2arc_write_max</code></a>
	      steuert, wie viele Bytes in den Cache pro Sekunde
	      geschrieben werden, während <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-l2arc_write_boost"><code class="varname">vfs.zfs.l2arc_write_boost</code></a>
	      zu diesem Limit während der <span class="quote">&#8222;<span class="quote">Turbo Warmup
	      Phase</span>&#8220;</span> hinzuaddiert wird (Write Boost).</td></tr><tr><td valign="top"><a id="zfs-term-zil"></a><acronym class="acronym">ZIL</acronym></td><td valign="top"><acronym class="acronym">ZIL</acronym> beschleunigt synchrone
	      Transaktionen durch die Verwendung von Speichermedien
	      wie <acronym class="acronym">SSD</acronym>s, welche schneller sind als
	      diejenigen, welche Teil des Speicherpools sind.  Wenn
	      eine Anwendung einen synchronen Schreibvorgang anfordert
	      (eine Garantie, dass die Daten sicher auf den Platten
	      gespeichert wurden anstatt nur zwischengespeichert zu
	      sein, um später geschrieben zu werden), werden die Daten
	      auf den schnelleren <acronym class="acronym">ZIL</acronym>-Speicher
	      geschrieben und dann später auf die regulären
	      Festplatten.  Dies reduziert die Latenz sehr und
	      verbessert die Geschwindigkeit.  Nur synchrone Vorgänge
	      wie die von Datenbanken werden durch den Einsatz eines
	      <acronym class="acronym">ZIL</acronym> profitieren.  Reguläre,
	      asynchrone Schreibvorgänge wie das Kopieren von Dateien
	      wird den <acronym class="acronym">ZIL</acronym> überhaupt nicht
	      verwenden.</td></tr><tr><td valign="top"><a id="zfs-term-cow"></a>Copy-On-Write</td><td valign="top">Im Gegensatz zu traditionellen Dateisystemen werden
	      beim Überschreiben von Daten bei <acronym class="acronym">ZFS</acronym>
	      die neuen Daten an einen anderen Block geschrieben,
	      anstatt die alten Daten an der gleichen Stelle zu
	      überschreiben.  Nur wenn dieser Schreibvorgang beendet
	      wurde, werden die Metadaten aktualisiert, um auf die
	      neue Position zu verweisen.  Im Falle eines kurzen
	      Schreibvorgangs (ein Systemabsturz oder Spannungsverlust
	      während eine Datei geschrieben wird) sind die gesamten
	      Inhalte der Originaldatei noch vorhanden und der
	      unvollständige Schreibvorgang wird verworfen.  Das
	      bedeutet auch, dass <acronym class="acronym">ZFS</acronym> nach einem
	      unvorhergesehenen Ausfall keinen <a class="citerefentry" href="http://www.FreeBSD.org/cgi/man.cgi?query=fsck&amp;sektion=8"><span class="citerefentry"><span class="refentrytitle">fsck</span>(8)</span></a>
	      benötigt.</td></tr><tr><td valign="top"><a id="zfs-term-dataset"></a>Dataset</td><td valign="top"><span class="emphasis"><em>Dataset</em></span> ist der generische
	      Begriff für ein <acronym class="acronym">ZFS</acronym>-Dateisystem,
	      Volume, Schnappschüsse oder Klone.  Jedes Dataset
	      besitzt einen eindeutigen Namen in der Form
	      <em class="replaceable"><code>poolname/path@snapshot</code></em>  Die
	      Wurzel des Pools ist technisch gesehen auch ein Dataset.
	      Kind-Datasets werden hierarchisch wie Verzeichnisse
	      benannt.  Beispielsweise ist
	      <em class="replaceable"><code>mypool/home</code></em> das
	      Heimatdataset, ein Kind von
	      <em class="replaceable"><code>mypool</code></em> und erbt die
	      Eigenschaften von diesem.  Dies kann sogar noch
	      erweitert werden durch das Erstellen von
	      <em class="replaceable"><code>mypool/home/user</code></em>.  Dieses
	      Enkelkind-Dataset wird alle Eigenschaften von den Eltern
	      und Großeltern erben.  Eigenschaften auf einem Kind
	      können die geerbten Standardwerte der Eltern und
	      Großeltern ändern und überschreiben.  Die Verwaltung
	      von Datasets und dessen Kindern lässt sich
	      <a class="link" href="zfs-zfs-allow.html" title="21.5. Delegierbare Administration">delegieren</a>.</td></tr><tr><td valign="top"><a id="zfs-term-filesystem"></a>Dateisystem</td><td valign="top">Ein <acronym class="acronym">ZFS</acronym>-Dataset wird meistens
	      als ein Dateisystem verwendet.  Wie jedes andere
	      Dateisystem kann auch ein
	      <acronym class="acronym">ZFS</acronym>-Dateisystem irgendwo in der
	      Verzeichnishierarchie eingehängt werden und enthält
	      seine eigenen Dateien und Verzeichnisse mit
	      Berechtigungen, Flags und anderen Metadaten.</td></tr><tr><td valign="top"><a id="zfs-term-volume"></a>Volume</td><td valign="top">Zusätzlich zu regulären Dateisystem-Datasets, kann
	      <acronym class="acronym">ZFS</acronym> auch Volumes erstellen, die
	      Blockgeräte sind.  Volumes besitzen viele der gleichen
	      Eigenschaften, inklusive copy-on-write, Schnappschüsse,
	      Klone und Prüfsummen.  Volumes sind nützlich, um andere
	      Dateisystemformate auf <acronym class="acronym">ZFS</acronym>
	      aufzusetzen, so wie <acronym class="acronym">UFS</acronym>
	      Virtualisierung, oder das Exportieren von
	      <acronym class="acronym">iSCSI</acronym>-Abschnitten.</td></tr><tr><td valign="top"><a id="zfs-term-snapshot"></a>Snapshot
	      (Schnappschuss)</td><td valign="top">Das <a class="link" href="zfs-term.html#zfs-term-cow">copy-on-write</a>
	      (<acronym class="acronym">COW</acronym>)-Entwicklung von
	      <acronym class="acronym">ZFS</acronym> erlaubt das Erstellen von beinahe
	      sofortigen, konsistenten Schnappschüssen mit beliebigen
	      Namen.  Nachdem ein Schnappschuss von einem Dataset
	      angelegt oder ein rekursiver Schnappschuss eines
	      Elterndatasets, welcher alle Kinddatasets enthält,
	      erstellt wurde, werden neue Daten auf neue Blöcke
	      geschrieben, jedoch die alten Blöcke nicht wieder als
	      freier Speicher zurückgewonnen.  Der Schnappschuss
	      enthält die Originalversion des Dateisystems und das
	      aktive Dateisystem besitzt alle Änderungen, die seit dem
	      Schnappschuss erstellt wurden.  Kein zusätzlicher Platz
	      wird benötigt.  Werden neue Daten auf das aktive
	      Dateisystem geschrieben, werden neue Blöcke allokiert,
	      um diese Daten zu speichern.  Die scheinbare Größe des
	      Schnappschusses wird wachsen, da die Blöcke nicht mehr
	      länger im aktiven Dateisystem, sondern nur noch im
	      Schnappschuss Verwendung finden.  Diese Schnappschüsse
	      können nur lesend eingehängt werden, um vorherige
	      Versionen von Dateien wiederherzustellen.  Ein <a class="link" href="zfs-zfs.html#zfs-zfs-snapshot" title="21.4.5. Verwalten von Schnappschüssen">rollback</a> eines
	      aktiven Dateisystems auf einen bestimmten Schnappschuss
	      ist ebenfalls möglich, was alle Änderungen, die seit dem
	      Anlegen des Schnappschusses vorgenommen wurden, wieder
	      Rückgängig macht.  Jeder Block im Pool besitzt einen
	      Referenzzähler, der verfolgt, wieviele Schnappschüsse,
	      Klone, Datasets oder Volumes diesen Block nutzen.  Wenn
	      Dateien und Schnappschüsse gelöscht werden, verringert
	      dies auch den Referenzzähler.  Wenn ein Block nicht mehr
	      länger referenziert wird, kann er als freier Speicher
	      wieder genutzt werden.  Schnappschüsse können auch mit
	      <a class="link" href="zfs-zfs.html#zfs-zfs-snapshot" title="21.4.5. Verwalten von Schnappschüssen">hold</a> markiert
	      werden.  Wenn versucht wird, einen solchen Schnappschuss
	      zu zerstören, wird stattdessen ein
	      <code class="literal">EBUSY</code>-Fehler ausgegeben.  Jeder
	      Schnappschuss kann mehrere holds besitzen, jeder mit
	      einem eindeutigen Namen.  Das Kommando <a class="link" href="zfs-zfs.html#zfs-zfs-snapshot" title="21.4.5. Verwalten von Schnappschüssen">release</a> entfernt
	      diese, damit der Schnappschuss gelöscht werden kann.
	      Schnappschüsse lassen sich auf Volumes ebenfalls
	      anlegen, allerdings können diese nur geklont oder
	      zurückgerollt werden, nicht jedoch unabhängig
	      eingehängt.</td></tr><tr><td valign="top"><a id="zfs-term-clone"></a>Clone (Klone)</td><td valign="top">Schnappschüsse können auch geklont werden.  Ein
	      Klon stellt eine veränderbare Version eines
	      Schnappschusses dar, was es ermöglicht, das Dateisystem
	      als neues Dataset aufzuspalten.  Genau wie bei einem
	      Schnappschuss verbraucht ein Klon keinen zusätzlichen
	      Platz.  Wenn neue Daten auf einen Klon geschrieben und
	      neue Blöcke allokiert werden, wächst auch die Größe des
	      Klons.  Wenn Blöcke im geklonten Dateisystem oder Volume
	      überschrieben werden, verringert sich auch der
	      Referenzzähler im vorherigen Block.  Der Schnappschuss,
	      auf dem der Klon basiert kann nicht gelöscht werden,
	      weil der Klon darauf eine Abhängigkeit besitzt.  Der
	      Schnappschuss stellt den Elternteil dar und der Klon das
	      Kind.  Klone lassen sich <span class="emphasis"><em>promoted</em></span>
	      (befördern), was die Abhängigkeit auflöst und den Klon
	      zum Elternteil macht und den vorherigen Elternteil das
	      Kind.  Diese Operation benötigt keinen zusätzlichen
	      Plattenplatz.  Da die Menge an verwendetem Speicher vom
	      Elternteil und dem Kind vertauscht wird, betrifft dies
	      eventuell vorhandene Quotas und Reservierungen.</td></tr><tr><td valign="top"><a id="zfs-term-checksum"></a>Checksum (Prüfsumme)</td><td valign="top">Jeder Block, der allokiert wird erhält auch eine
	      Prüfsumme.  Der verwendete Prüfsummenalgorithmus ist
	      eine Eigenschaft jedes Datasets, siehe dazu <a class="link" href="zfs-zfs.html#zfs-zfs-set" title="21.4.4. Festlegen von Dataset-Eigenschaften"><code class="command">set</code></a>.
	      Die Prüfsumme jedes Blocks wird transparent validiert
	      wenn er gelesen wird, was es <acronym class="acronym">ZFS</acronym>
	      ermöglicht, stille Verfälschung zu entdecken.  Wenn die
	      gelesenen Daten nicht mit der erwarteten Prüfsumme
	      übereinstimmen, wird <acronym class="acronym">ZFS</acronym> versuchen,
	      die Daten aus jeglicher verfügbarer Redundanz (wie
	      Spiegel oder <acronym class="acronym">RAID-Z</acronym>) zu
	      rekonstruieren.  Eine Überprüfung aller Prüfsummen kann
	      durch das Kommando <a class="link" href="zfs-term.html#zfs-term-scrub"><code class="command">scrub</code></a>
	      ausgelöst werden.
	      Prüfsummenalgorithmen sind:

	      <div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><code class="literal">fletcher2</code></p></li><li class="listitem"><p><code class="literal">fletcher4</code></p></li><li class="listitem"><p><code class="literal">sha256</code></p></li></ul></div>

	      Die <code class="literal">fletcher</code>-Algorithmen sind
	      schneller, aber dafür ist <code class="literal">sha256</code> ein
	      starker kryptographischer Hash und besitzt eine viel
	      niedrigere Chance auf Kollisionen zu stoßen mit dem
	      Nachteil geringerer Geschwindigkeit.  Prüfsummen können
	      deaktiviert werden, dies wird aber nicht
	      empfohlen.</td></tr><tr><td valign="top"><a id="zfs-term-compression"></a>Compression</td><td valign="top">Jedes Dataset besitzt eine compression-Eigenschaft,
	      die standardmäßig ausgeschaltet ist.  Diese Eigenschaft
	      kann auf eine Reihe von Kompressionsalgorithmen
	      eingestellt werden.  Dadurch werden alle neuen Daten,
	      die auf das Dataset geschrieben werden, komprimiert.
	      Neben einer Reduzierung von verbrauchtem Speicher wird
	      oft der Lese- und Schreibdurchsatz erhöht, weil weniger
	      Blöcke gelesen oder geschrieben werden müssen.

	      <div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><a id="zfs-term-compression-lz4"></a><span class="emphasis"><em><acronym class="acronym">LZ4</acronym></em></span> -
		    Wurde in der <acronym class="acronym">ZFS</acronym> Poolversion
		    5000 (feature flags) hinzugefügt und
		    <acronym class="acronym">LZ4</acronym> ist jetzt der empfohlene
		    Kompressionsalgorithmus.  <acronym class="acronym">LZ4</acronym>
		    komprimiert ungefähr 50% schneller als
		    <acronym class="acronym">LZJB</acronym>, wenn er auf
		    komprimierbaren Daten angewendet wird und ist über
		    dreimal schneller, wenn unkomprimierbare Daten
		    vorliegen.  <acronym class="acronym">LZ4</acronym> entkomprimiert
		    auch ungefähr 80% schneller als
		    <acronym class="acronym">LZJB</acronym>.  Auf modernen
		    <acronym class="acronym">CPU</acronym>s, kann
		    <acronym class="acronym">LZ4</acronym> oft über 500 MB/s
		    komprimieren und entkomprimiert (pro einzelnem
		    CPU-Kern) bei über 1.5 GB/s.</p><div xmlns="" class="note"><h3 class="admontitle">Anmerkung: </h3><p xmlns="http://www.w3.org/1999/xhtml"><acronym class="acronym">LZ4</acronym>-Komprimierung ist nur
		      verfügbar nach FreeBSD 9.2.</p></div></li><li class="listitem"><p><a id="zfs-term-compression-lzjb"></a><span class="emphasis"><em><acronym class="acronym">LZJB</acronym></em></span> -
		    Der Standardkompressionsalgorithmus wurde von
		    Jeff Bonwick, einem der ursprünglichen Entwickler
		    von <acronym class="acronym">ZFS</acronym>, entworfen.
		    <acronym class="acronym">LZJB</acronym> bietet gute Komprimierung
		    mit weniger <acronym class="acronym">CPU</acronym>-Überhang im
		    Vergleich zu <acronym class="acronym">GZIP</acronym>.  In der
		    Zukunft wird der Standardkompressionsalgorithmus
		    wahrscheinlich auf <acronym class="acronym">LZ4</acronym>
		    gewechselt.</p></li><li class="listitem"><p><a id="zfs-term-compression-gzip"></a><span class="emphasis"><em><acronym class="acronym">GZIP</acronym></em></span> -
		    Ein populärer Stromkompressionsalgorithmus ist
		    auch in <acronym class="acronym">ZFS</acronym> verfügbar.  Einer
		    der Hauptvorteile von der Verwendung von
		    <acronym class="acronym">GZIP</acronym> ist seine konfigurierbare
		    Komprimierungsstufe.  Wenn die Eigenschaft
		    <code class="literal">compress</code> gesetzt wird, kann der
		    Administrator die Stufe der Komprimierung wählen,
		    die von <code class="literal">gzip1</code>, der kleinsten
		    Komprimierungsstufe, bis zu
		    <code class="literal">gzip9</code>, der höchsten
		    Komprimierungsstufe, reicht.  Dies erlaubt es dem
		    Administrator zu steuern, wieviel
		    <acronym class="acronym">CPU</acronym>-Zeit für eingesparten
		    Plattenplatz eingetauscht werde soll.</p></li><li class="listitem"><p><a id="zfs-term-compression-zle"></a><span class="emphasis"><em><acronym class="acronym">ZLE</acronym></em></span> -
		    Zero Length Encoding ist ein besonderer
		    Kompressionsalgorithmus, welcher nur fortlaufende
		    Aneinanderreihungen von Nullen komprimiert.
		    Dieser Komprimierungsalgorithmus ist nur sinnvoll,
		    wenn das Dataset viele große Blöcke von Nullen
		    aufweist.</p></li></ul></div></td></tr><tr><td valign="top"><a id="zfs-term-copies"></a>Copies</td><td valign="top">Wenn die Eigenschaft <code class="literal">copies</code> auf
	      einen Wert grösser als 1 gesetzt wird, weist das
	      <acronym class="acronym">ZFS</acronym> an, mehrere Kopien eines Blocks
	      im <a class="link" href="zfs-term.html#zfs-term-filesystem">Dateisystem</a>
	      oder
	      <a class="link" href="zfs-term.html#zfs-term-volume">Volume</a> anzulegen.
	      Diese Eigenschaft auf einem wichtigen Dataset
	      einzustellen sorgt für zusätzliche Redundanz, aus der
	      ein Block wiederhergestellt werden kann, der nicht mehr
	      mit seiner Prüfsumme übereinstimmt.  In Pools ohne
	      Redundanz ist die copies-Eigenschaft die einzige Form
	      von Redundanz.  Die Eigenschaft kann einen einzelnen
	      schlechten Sektor oder andere Formen von kleineren
	      Verfälschungen wiederherstellen, schützt jedoch nicht
	      den Pool vom Verlust einer gesamten Platte.</td></tr><tr><td valign="top"><a id="zfs-term-deduplication"></a>Deduplizierung</td><td valign="top">Prüfsummen ermöglichen es, Duplikate von Blöcken zu
	      erkennen, wenn diese geschrieben werden.  Mit
	      Deduplizierung erhöht sich der Referenzzähler eines
	      existierenden, identischen Blocks, was Speicherplatz
	      einspart.  Um Blockduplikate zu erkennen, wird im
	      Speicher eine Deduplizierungstabelle
	      (<acronym class="acronym">DDT</acronym>) geführt.  Die Tabelle enthält
	      eine Liste von eindeutigen Prüfsummen, die Position
	      dieser Blöcke und einen Referenzzähler.  Werden neue
	      Daten geschrieben, wird die Prüfsumme berechnet und mit
	      der Liste verglichen.  Wird eine Übereinstimmung
	      gefunden, wird der existierende Block verwendet.  Der
	      <acronym class="acronym">SHA256</acronym>-Prüfsummenalgorithmus wird mit
	      Deduplizierung benutzt, um einen sicheren
	      kryptographischen Hash zu bieten.  Deduplizierung lässt
	      sich konfigurieren.  Wenn <code class="literal">dedup</code> auf
	      <code class="literal">on</code> steht, wird angenommen, dass eine
	      übereinstimmende Prüfsumme bedeutet, dass die Daten
	      identisch sind.  Steht <code class="literal">dedup</code> auf
	      <code class="literal">verify</code>, werden die Daten in den
	      beiden Blöcken Byte für Byte geprüft, um
	      sicherzustellen, dass diese wirklich identisch sind.
	      Wenn die Daten nicht identisch sind, wird die Kollision
	      im Hash vermerkt und die beiden Blöcke separat
	      gespeichert.  Da die <acronym class="acronym">DDT</acronym> den Hash
	      jedes einzigartigen Blocks speichern muss, benötigt sie
	      eine große Menge an Speicher.  Eine generelle
	      Faustregel besagt, dass 5-6 GB RAM pro 1 TB
	      deduplizierter Daten benötigt werden.  In Situationen,
	      in denen es nicht praktikabel ist, genug
	      <acronym class="acronym">RAM</acronym> vorzuhalten, um die gesamte
	      <acronym class="acronym">DDT</acronym> im Speicher zu belassen, wird die
	      Geschwindigkeit stark darunter leiden, da die
	      <acronym class="acronym">DDT</acronym> von der Platte gelesen werden
	      muss, bevor jeder neue Block geschrieben wird.
	      Deduplizierung kann den <acronym class="acronym">L2ARC</acronym> nutzen,
	      um die <acronym class="acronym">DDT</acronym> zu speichern, was einen
	      guten Mittelweg zwischen schnellem Systemspeicher und
	      langsameren Platten darstellt.  Bedenken Sie, dass durch
	      die Verwendung von Komprimierung meistens genauso große
	      Platzersparnis möglich ist, ohne den zusätzlichen
	      Hauptspeicherplatzbedarf.</td></tr><tr><td valign="top"><a id="zfs-term-scrub"></a>Scrub (Bereinigung)</td><td valign="top">Anstatt einer Konsistenzprüfung wie <a class="citerefentry" href="http://www.FreeBSD.org/cgi/man.cgi?query=fsck&amp;sektion=8"><span class="citerefentry"><span class="refentrytitle">fsck</span>(8)</span></a>
	      verwendet <acronym class="acronym">ZFS</acronym>
	      <code class="command">scrub</code>.  <code class="command">scrub</code>
	      liest alle Datenblöcke, die auf dem Pool gespeichert
	      sind und prüft deren Prüfsumme gegen die als richtig
	      in den Metadaten gespeicherte Prüfsumme.  Eine
	      periodische Prüfung aller im Pool gespeicherten Daten
	      versichert, dass verfälschte Blöcke rekonstruiert werden
	      können, bevor dies nötig ist.  Ein Scrub wird nicht nach
	      einem unsauberen Herunterfahren benötigt, wird jedoch
	      einmal alle drei Monate angeraten.  Die Prüfsumme von
	      jedem Block wird verifiziert, wenn Blöcke während des
	      normalen Betriebs gelesen werden, jedoch stellt ein
	      Scrub sicher, dass sogar weniger häufig verwendete
	      Blöcke auf stille Verfälschungen hin untersucht werden.
	      Datenintegrität wird dadurch erhöht, besonders wenn es
	      sich um Archivspeichersituationen handelt.  Die relative
	      Priorität des <code class="command">scrub</code> lässt sich mit
	      <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-scrub_delay"><code class="varname">vfs.zfs.scrub_delay</code></a>
	      anpassen, um zu verhindern, dass der scrub die
	      Geschwindigkeit von anderen Anfragen auf dem Pool
	      beeinträchtigt.</td></tr><tr><td valign="top"><a id="zfs-term-quota"></a>Dataset Quotas</td><td valign="top"><acronym class="acronym">ZFS</acronym> bietet sehr schnelle und
	      akkurate Dataset-, Benutzer- und
	      Gruppenspeicherplatzbuchhaltung, zusätzlich zu Quotas
	      und Speicherplatzreservierungen.  Dies gibt dem
	      Administrator feingranulare Kontrolle darüber, wie
	      Speicherplatz allokiert und die Reservierung für
	      kritische Dateisysteme vorgenommen wird

	      <p><acronym class="acronym">ZFS</acronym> unterstützt verschiedene
		Arten von Quotas: die Dataset-Quota, die <a class="link" href="zfs-term.html#zfs-term-refquota">Referenzquota
		(<acronym class="acronym">refquota</acronym>)</a>, die <a class="link" href="zfs-term.html#zfs-term-userquota">Benutzerquota</a> und
		die <a class="link" href="zfs-term.html#zfs-term-groupquota">Gruppenquota</a>
		sind verfügbar.</p>

	      <p>Quotas beschränken die Menge an Speicherplatz,
		welche ein Dataset, seine Kinder, einschließlich
		Schnappschüsse des Datasets, deren Kinder und die
		Schnappschüsse von diesen Datasets, verbrauchen
		können.</p>

	      <div xmlns="" class="note"><h3 class="admontitle">Anmerkung: </h3><p xmlns="http://www.w3.org/1999/xhtml">Quotas können nicht auf Volumes gesetzt werden,
		  da die Eigenschaft <code class="literal">volsize</code> als
		  eine implizite Quota agiert.</p></div></td></tr><tr><td valign="top"><a id="zfs-term-refquota"></a>Referenzquota</td><td valign="top">Ein Referenzquota beschränkt die Menge an
	      Speicherplatz, die ein Dataset verbrauchen kann durch
	      das Erzwingen einer harten Grenze.  Jedoch beinhaltet
	      diese harte Grenze nur Speicherplatz, die das Dataset
	      referenziert und beinhaltet nicht den Speicher, der von
	      Kindern, wie Dateisystemen oder Schnappschüssen,
	      verbraucht wird.</td></tr><tr><td valign="top"><a id="zfs-term-userquota"></a>Benutzerquota</td><td valign="top">Benutzerquotas sind hilfreich, um die Menge an
	      Speicherplatz, die ein bestimmter Benutzer verbrauchen
	      kann, einzuschränken.</td></tr><tr><td valign="top"><a id="zfs-term-groupquota"></a>Gruppenquota</td><td valign="top">Die Gruppenquota beschränkt die Menge an
	      Speicherplatz, die eine bestimmte Gruppe verbrauchen
	      darf.</td></tr><tr><td valign="top"><a id="zfs-term-reservation"></a>Dataset-Reservierung</td><td valign="top">Die Eigenschaft <code class="literal">reservation</code>
	      ermöglicht es, ein Minimum an Speicherplatz für ein
	      bestimmtes Dataset und dessen Kinder zu garantieren.
	      Wenn eine Reservierung von 10 GB auf
	      <code class="filename">storage/home/bob</code> gesetzt ist und
	      ein anderes Dataset versucht, allen freien Speicherplatz
	      zu verwenden, bleiben zumindest noch 10 GB an
	      Speicher reserviert.  Wenn von
	      <code class="filename">storage/home/bob</code> ein Schnappschuss
	      angelegt wird, wird dieser von der Reservierung
	      abgezogen und zählt damit dagegen.
	      Die Eigenschaft <a class="link" href="zfs-term.html#zfs-term-refreservation"><code class="literal">refreservation</code></a>
	      funktioniert auf ähnliche Weise, jedoch
	      <span class="emphasis"><em>exkludiert</em></span> diese Kinder wie
	      Schnappschüsse.

	      <p>Reservierungen jeder Art sind in vielen
		Situationen nützlich, so wie bei der Planung und dem
		Testen der richtigen Speicherplatzallokation in einem
		neuen System oder durch die Zusicherung, dass genug
		Speicherplatz auf Dateisystemen für Audio-Logs oder
		Systemwiederherstellungsprozeduren und Dateien
		verfügbar ist.</p>
	    </td></tr><tr><td valign="top"><a id="zfs-term-refreservation"></a>Referenzreservierung</td><td valign="top">Die Eigenschaft <code class="literal">refreservation</code>
	      ermöglicht es, ein Minimum an Speicherplatz für die
	      Verwendung eines bestimmten Datasets zu garantieren,
	      <span class="emphasis"><em>exklusiv</em></span> dessen Kinder.  Das
	      bedeutet, dass wenn eine 10 GB-Reservierung auf
	      <code class="filename">storage/home/bob</code> vorhanden ist und
	      ein anderes Dataset versucht, alle freien Speicherplatz
	      aufzubrauchen, sind zumindest noch
	      10 GB Speicher reserviert.  Im Gegensatz zu einer
	      regulären <a class="link" href="zfs-term.html#zfs-term-reservation">Reservierung</a> wird
	      der Speicher von Schnappschüssen und Kinddataset nicht
	      gegen die Reservierung gezählt.  Beispielsweise, wenn
	      ein Schnappschuss von
	      <code class="filename">storage/home/bob</code> angelegt wird,
	      muss genug Plattenplatz außerhalb der Menge an
	      <code class="literal">refreservation</code> vorhanden sein, damit
	      die Operation erfolgreich durchgeführt wird.  Kinder des
	      Hauptdatasets werden nicht in die Menge an
	      <code class="literal">refreservation</code> gezählt und dringen
	      auf diese Weise auch nicht in den gesetzten Speicher
	      ein.</td></tr><tr><td valign="top"><a id="zfs-term-resilver"></a>Resilver</td><td valign="top">Wenn eine Platte ausfällt und ersetzt wird, muss
	      die neue Platte mit den Daten gefüllt werden, die
	      verloren gegangen sind.  Der Prozess der Verwendung der
	      Paritätsinformationen, welche über die übrigen Platten
	      verteilt sind, um die fehlenden Daten zu berechnen und
	      auf die neue Platte zu übertragen, wird
	      <span class="emphasis"><em>resilvering</em></span> genannt.</td></tr><tr><td valign="top"><a id="zfs-term-online"></a>Online</td><td valign="top">Ein Pool oder vdev im Zustand
	      <code class="literal">Online</code> besitzt alle verbundenen
	      Mitgliedsgeräte und ist voll funktionsfähig.
	      Individuelle Geräte im Zustand
	      <code class="literal">Online</code> funktionieren normal.</td></tr><tr><td valign="top"><a id="zfs-term-offline"></a>Offline</td><td valign="top">Individuelle Geräte lassen sich vom Administrator
	      in den Zustand <code class="literal">Offline</code> versetzen,
	      wenn es ausreichend Redundanz gibt, um zu verhindern,
	      dass der Pool oder das vdev in den Zustand
	      <a class="link" href="zfs-term.html#zfs-term-faulted">Faulted</a> versetzt
	      wird.  Ein Administrator kann eine Platte vor einem
	      Austausch offline nehmen oder um es leichter zu machen,
	      diese zu identifizieren.</td></tr><tr><td valign="top"><a id="zfs-term-degraded"></a>Degraded</td><td valign="top">Ein Pool oder vdev im Zustand
	      <code class="literal">Degraded</code> hat eine oder mehrere
	      Platten, welche getrennt wurden oder ausgefallen sind.
	      Der Pool kann immer noch verwendet werden, doch wenn
	      noch weitere Geräte ausfallen, kann der Pool nicht
	      wiederhergestellt werden.  Die fehlenden Geräte
	      anzuschließen oder die defekten Platten zu ersetzen
	      wird den Pool wieder in den Zustand
	      <a class="link" href="zfs-term.html#zfs-term-online">Online</a> versetzen,
	      nachdem die angeschlossenen oder neuen Geräte den
	      <a class="link" href="zfs-term.html#zfs-term-resilver">Resilver</a>-Prozess
	      abgeschlossen haben.</td></tr><tr><td valign="top"><a id="zfs-term-faulted"></a>Faulted</td><td valign="top">Ein Pool oder vdev im Zustand
	      <code class="literal">Faulted</code> funktioniert nicht länger.
	      Die Daten darauf sind nicht mehr länger verfügbar.  Ein
	      Pool oder vdev geht in den Zustand
	      <code class="literal">Faulted</code> über, wenn die Anzahl der
	      fehlenden oder defekten Geräte die Redundanzstufe im
	      vdev überschreiten.  Wenn fehlende Geräte angeschlossen
	      werden, geht der Pool wieder in den Zustand <a class="link" href="zfs-term.html#zfs-term-online">Online</a>.  Wenn es nicht
	      genügend Redundanz gibt, um die Anzahl an defekten
	      Platten zu kompensieren, sind die Inhalte des Pools
	      verloren und müssen von der Sicherung wiederhergestellt
	      werden.</td></tr></tbody></table></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="zfs-links.html">Zurück</a> </td><td width="20%" align="center"><a accesskey="u" href="zfs.html">Nach oben</a></td><td width="40%" align="right"> <a accesskey="n" href="filesystems.html">Weiter</a></td></tr><tr><td width="40%" align="left" valign="top">21.7. Zusätzliche Informationen </td><td width="20%" align="center"><a accesskey="h" href="index.html">Zum Anfang</a></td><td width="40%" align="right" valign="top"> Kapitel 22. Dateisystemunterstützung</td></tr></table></div><p xmlns="http://www.w3.org/TR/xhtml1/transitional" align="center"><small>Wenn Sie Fragen zu FreeBSD haben, schicken Sie eine E-Mail an
    &lt;<a href="mailto:de-bsd-questions@de.FreeBSD.org">de-bsd-questions@de.FreeBSD.org</a>&gt;.<br></br>
    Wenn Sie Fragen zu dieser Dokumentation haben, schicken Sie eine E-Mail an
    &lt;<a href="mailto:de-bsd-translators@de.FreeBSD.org">de-bsd-translators@de.FreeBSD.org</a>&gt;.</small></p></body></html>